{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe51c5-7d83-4ae0-a1be-d46a4547a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "# Load the wine quality dataset\n",
    "data = pd.read_csv(\n",
    "    filepath_or_buffer=\"data/raw_csv/winequality-white.csv\",\n",
    "    sep=\";\"\n",
    ")\n",
    "\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(\"Splitting data...\")\n",
    "\n",
    "# Create train/validation/test splits\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_x = train.drop([\"quality\"], axis=1).values\n",
    "train_y = train[[\"quality\"]].values.ravel()\n",
    "test_x = test.drop([\"quality\"], axis=1).values\n",
    "test_y = test[[\"quality\"]].values.ravel()\n",
    "\n",
    "print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "\n",
    "# Further split training data for validation\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(\n",
    "    train_x, train_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "print(train.head())\n",
    "\n",
    "# Create model signature for deployment\n",
    "signature = infer_signature(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a459b2-04e1-4e3a-aa02-1c2c48b5f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(learning_rate, momentum, epochs=10):\n",
    "    \"\"\"\n",
    "    Create and train a neural network with specified hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training results including model and metrics\n",
    "    \"\"\"\n",
    "    # Normalize input features for better training stability\n",
    "    mean = np.mean(train_x, axis=0)\n",
    "    var = np.var(train_x, axis=0)\n",
    "\n",
    "    # Define model architecture\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.Input([train_x.shape[1]]),\n",
    "            keras.layers.Normalization(mean=mean, variance=var),\n",
    "            keras.layers.Dense(64, activation=\"relu\"),\n",
    "            keras.layers.Dropout(0.2),  # Add regularization\n",
    "            keras.layers.Dense(32, activation=\"relu\"),\n",
    "            keras.layers.Dense(1),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Compile with specified hyperparameters\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
    "    )\n",
    "\n",
    "    # Train with early stopping for efficiency\n",
    "    early_stopping = keras.callbacks.EarlyStopping(\n",
    "        patience=3, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        validation_data=(valid_x, valid_y),\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0,  # Reduce output for cleaner logs\n",
    "    )\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_rmse = model.evaluate(valid_x, valid_y, verbose=0)\n",
    "\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"val_rmse\": val_rmse,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"history\": history,\n",
    "        \"epochs_trained\": len(history.history[\"loss\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b395d-b142-468e-9f31-69a456683469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"\n",
    "    Objective function for hyperparameter optimization.\n",
    "    This function will be called by Hyperopt for each trial.\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Log hyperparameters being tested\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"learning_rate\": params[\"learning_rate\"],\n",
    "                \"momentum\": params[\"momentum\"],\n",
    "                \"optimizer\": \"SGD\",\n",
    "                \"architecture\": \"64-32-1\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Train model with current hyperparameters\n",
    "        result = create_and_train_model(\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            momentum=params[\"momentum\"],\n",
    "            epochs=15,\n",
    "        )\n",
    "\n",
    "        # Log training results\n",
    "        mlflow.log_metrics(\n",
    "            {\n",
    "                \"val_rmse\": result[\"val_rmse\"],\n",
    "                \"val_loss\": result[\"val_loss\"],\n",
    "                \"epochs_trained\": result[\"epochs_trained\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Log the trained model\n",
    "        mlflow.tensorflow.log_model(result[\"model\"], name=\"model\", signature=signature)\n",
    "\n",
    "        # Log training curves as artifacts\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(result[\"history\"].history[\"loss\"], label=\"Training Loss\")\n",
    "        plt.plot(result[\"history\"].history[\"val_loss\"], label=\"Validation Loss\")\n",
    "        plt.title(\"Model Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(\n",
    "            result[\"history\"].history[\"root_mean_squared_error\"], label=\"Training RMSE\"\n",
    "        )\n",
    "        plt.plot(\n",
    "            result[\"history\"].history[\"val_root_mean_squared_error\"],\n",
    "            label=\"Validation RMSE\",\n",
    "        )\n",
    "        plt.title(\"Model RMSE\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"training_curves.png\")\n",
    "        mlflow.log_artifact(\"training_curves.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Return loss for Hyperopt (it minimizes)\n",
    "        return {\"loss\": result[\"val_rmse\"], \"status\": STATUS_OK}\n",
    "\n",
    "\n",
    "# Define search space for hyperparameters\n",
    "search_space = {\n",
    "    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(1e-5), np.log(1e-1)),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.0, 0.9),\n",
    "}\n",
    "\n",
    "print(\"Search space defined:\")\n",
    "print(\"- Learning rate: 1e-5 to 1e-1 (log-uniform)\")\n",
    "print(\"- Momentum: 0.0 to 0.9 (uniform)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97161d9-03b1-460e-98e8-0bdd30388629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or set experiment\n",
    "experiment_name = \"wine-quality-optimization\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"Starting hyperparameter optimization experiment: {experiment_name}\")\n",
    "print(\"This will run 15 trials to find optimal hyperparameters...\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"hyperparameter-sweep\"):\n",
    "    # Log experiment metadata\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"optimization_method\": \"Tree-structured Parzen Estimator (TPE)\",\n",
    "            \"max_evaluations\": 15,\n",
    "            \"objective_metric\": \"validation_rmse\",\n",
    "            \"dataset\": \"wine-quality\",\n",
    "            \"model_type\": \"neural_network\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Run optimization\n",
    "    trials = Trials()\n",
    "    best_params = fmin(\n",
    "        fn=objective,\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=15,\n",
    "        trials=trials,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Find and log best results\n",
    "    best_trial = min(trials.results, key=lambda x: x[\"loss\"])\n",
    "    best_rmse = best_trial[\"loss\"]\n",
    "\n",
    "    # Log optimization results\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"best_learning_rate\": best_params[\"learning_rate\"],\n",
    "            \"best_momentum\": best_params[\"momentum\"],\n",
    "        }\n",
    "    )\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"best_val_rmse\": best_rmse,\n",
    "            \"total_trials\": len(trials.trials),\n",
    "            \"optimization_completed\": 1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd69a72-8569-4a71-8a95-a3a5529cbdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
